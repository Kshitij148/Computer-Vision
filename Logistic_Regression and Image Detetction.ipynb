{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision  #conatains utilities to work with image data \n",
    "#and also helps to download and import popular datasets\n",
    "from torchvision.datasets import MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the training dataset\n",
    "dataset=MNIST(root='data/',download=True)\n",
    "#it downloads th data to the data/ directory next to the notebook and creates Pytorch dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The datset has 60,000 images which can be used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)  #length of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset\n",
    "test_dataset=MNIST(root='data/',train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)  #length of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x18E156D2B80>, 5)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset[0]\n",
    "\n",
    "# (a 28 by 28 pixel image which is an object of class PIL,label for the image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "#called IPython magic commands\n",
    "#%matplotlib inline is added to indicate to jupyter that we want to plot the commands within the notebook\n",
    "#without this jupyter will show image in a popup\n",
    "\n",
    "#matplotlib imaging and graphing library in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,label=dataset[0]\n",
    "plt.imshow(image,cmap=\"gray\")\n",
    "#cmap is telling imshow that the image is grayscale\n",
    "# As it is a PIL image it can be displayed in the notebook using display(pil image)\n",
    "print(\"Label\",label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,label=dataset[10]\n",
    "plt.imshow(image,cmap='gray')\n",
    "print(\"Label\",label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorchdoesnt know how to work with images.We need to convert the images to tensors\n",
    "#we can do this by specifying a transform while createing our dataset.\n",
    "\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchvision.transforms contain many such predefined functions \n",
    "#we will use ToTensor transform to convert images into PyTorch tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#MNIST dataset(images and labels)\n",
    "dataset=MNIST(root='data/',train=True,transform=transforms.ToTensor())\n",
    "\n",
    "#converts the training dataset containing images to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor,label=dataset[0]\n",
    "print(img_tensor.shape,label)\n",
    "\n",
    "#The image is now converted to 1*28*28 tensor\n",
    "#as the image was 28*28 pixels\n",
    "#The first dimension is used to keep track of the color channels\n",
    "#MNIST dataset are grayscale so there's just one channel.\n",
    "#RGB dataset will have 3 channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[:,10:15,10:15])\n",
    "#here we are using array slicing\n",
    "#everything from first dimension in this case there is only one\n",
    "#5 rows  and  5 columns that are between 10 and 15 where 10 is inclusive 15 is exclusive\n",
    "print(torch.max(img_tensor),torch.min(img_tensor))\n",
    "#maximum and minimum values present in the tensor\n",
    "#the numbers in the tensors are pixel values where 0 represents black and 1 represents white\n",
    "#and the values in between are different shades of grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the image by passing in the selected rows and columns\n",
    "plt.imshow(img_tensor[0,10:15,10:15],cmap=\"gray\");\n",
    "\n",
    "#in simple terms array is like tensor([for colour channel[for 28 by 28 pixels[array1],[array2],.........[array28]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "#used to split the training dataset into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds,val_ds=random_split(dataset,[50000,10000])\n",
    "len(train_ds),len(val_ds)\n",
    "\n",
    "#random_split(dataset,[amount of data in split1,amount of data in split2])\n",
    "#random split is important to prevent overfitting of model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "train_loader=DataLoader(train_ds,batch_size,shuffle=True)\n",
    "val_loader=DataLoader(val_ds,batch_size)\n",
    "\n",
    "#dataloader is used to make batches each of size 128 rows\n",
    "#shuffle=True helps to randomize and generalize the training process\n",
    "\n",
    "#the validation dataset is used for evaluating the model  so no need of shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pred=x @ w.t() + b \n",
    "\n",
    "#nn.Linear(number of inputs,number of outputs)\n",
    "#Since nn.Linear excepts the each training example to be a vector each 1*28*28 image tensor\n",
    "#needs to be flattened out into a vector of size 784 (28*28) before being passed into the model \n",
    "\n",
    "#The output for each image is the vector of size 10 with each element of the vector signifying the probability a particular label\n",
    "#(0 to 9) . The predicted label for an image is simply the one with the highest probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size=28*28\n",
    "num_classes=10   #return probabilities for each label and maximum will be the predicted label\n",
    "\n",
    "#Logistic Regression model\n",
    "\n",
    "model=nn.Linear(input_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0265, -0.0225,  0.0041,  ...,  0.0127, -0.0042,  0.0020],\n",
       "        [-0.0330, -0.0002, -0.0011,  ..., -0.0235,  0.0094,  0.0216],\n",
       "        [ 0.0196, -0.0176,  0.0079,  ...,  0.0225, -0.0216,  0.0251],\n",
       "        ...,\n",
       "        [-0.0338,  0.0243, -0.0241,  ...,  0.0342, -0.0153, -0.0219],\n",
       "        [ 0.0115, -0.0046,  0.0354,  ...,  0.0245, -0.0218,  0.0172],\n",
       "        [ 0.0232, -0.0312, -0.0338,  ...,  0.0329, -0.0025, -0.0307]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight                \n",
    "#the weight tensor\n",
    "#this means tensor([ [784 elements], [784 elements],.......such 10 arrays])\n",
    "#all automatically generated (randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0269,  0.0027,  0.0197, -0.0264,  0.0078,  0.0238,  0.0052, -0.0197,\n",
       "        -0.0214,  0.0172], requires_grad=True)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias\n",
    "#10 bias values\n",
    "#all automatically generated (randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 9, 7, 5, 0, 1, 6, 6, 1, 0, 3, 0, 0, 4, 0, 0, 3, 1, 0, 8, 1, 5, 0, 4,\n",
      "        6, 3, 7, 4, 1, 1, 2, 7, 1, 5, 2, 6, 4, 3, 2, 3, 8, 5, 0, 1, 3, 0, 2, 5,\n",
      "        3, 4, 2, 8, 6, 4, 1, 7, 2, 6, 1, 2, 3, 5, 1, 5, 8, 2, 7, 4, 2, 6, 8, 0,\n",
      "        3, 5, 3, 4, 7, 2, 7, 8, 6, 2, 2, 4, 7, 0, 5, 4, 9, 4, 2, 5, 5, 1, 9, 1,\n",
      "        9, 6, 2, 0, 4, 7, 1, 2, 4, 2, 0, 7, 5, 0, 5, 6, 8, 5, 6, 3, 1, 7, 9, 6,\n",
      "        6, 0, 7, 9, 3, 2, 7, 5])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-284-75b684a5155b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#[batch size,1,28,28] will represent in this format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "for images,labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)  #[batch size,1,28,28] will represent in this format\n",
    "    output=model(images)\n",
    "    break\n",
    "    \n",
    "#this leads to error because pur input data does not have the right shape\n",
    "#Our images are of the shape 1*28*28 but we need them to be vectors of size 784 that is\n",
    "#flatten them out.\n",
    "\n",
    "#the models expects a vector of size 784 and we gave a 3 dimentional tensor so this created the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.reshape method of a tensor will allow us to efficiently view each \n",
    "#image as a flat vector \n",
    "\n",
    "#to include this additional functionality within our model we need to define a custom model by\n",
    "#extending the nn.Module class from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):  #this class extends nn.Module\n",
    "    def __init__(self):       #the constructor\n",
    "        super().__init__()     #using super method to call constructor of nn.Module\n",
    "        self.linear=nn.Linear(input_size,num_classes)\n",
    "        #inside the constructor we instantiate the weights and biases\n",
    "        \n",
    "    def forward(self,xb):       #this method gets executed when we pass some input to the model\n",
    "        xb=xb.reshape(-1,784)\n",
    "        out=self.linear(xb)     #this invokes the above nn.Linear  \n",
    "        #this is similar to model(inputs) and it will return the output that is the prediction\n",
    "        return out\n",
    "    \n",
    "        #reshape(128,784) that is batch size is 128 and each element in batch is a vetor of size 784\n",
    "        #using reshape(-1,784) telling pytorch to figure out what the batch size is dynamically\n",
    "        #so that it works for any batch size\n",
    "        \n",
    "model=MnistModel()  #creating an object of the class \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model has no longer .weight and .bias attriutes\n",
    "#as they are inside the .linear attribute\n",
    "#but it does have a .parameters method which returns the weights and bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0209, -0.0327,  0.0034,  ..., -0.0038,  0.0045, -0.0324],\n",
       "         [ 0.0218, -0.0044,  0.0076,  ...,  0.0030,  0.0063,  0.0180],\n",
       "         [-0.0085,  0.0212, -0.0181,  ...,  0.0018, -0.0346, -0.0279],\n",
       "         ...,\n",
       "         [-0.0344,  0.0065, -0.0068,  ...,  0.0233, -0.0008, -0.0068],\n",
       "         [-0.0176,  0.0250,  0.0117,  ...,  0.0010, -0.0044, -0.0352],\n",
       "         [-0.0136, -0.0250,  0.0062,  ..., -0.0197,  0.0119,  0.0061]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0216, -0.0198,  0.0074,  0.0303,  0.0159,  0.0201, -0.0311, -0.0183,\n",
       "          0.0256,  0.0017], requires_grad=True)]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape,model.linear.bias.shape)\n",
    "list(model.parameters())  #contains weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([128, 1, 28, 28])\n",
      "output.shape: torch.Size([128, 10])\n",
      "Sample outputs:\n",
      " tensor([[-0.2006,  0.3234,  0.0023, -0.0086, -0.3566,  0.1881,  0.3750,  0.0525,\n",
      "         -0.0327,  0.0806],\n",
      "        [ 0.2168,  0.0146, -0.0093, -0.2380, -0.3442,  0.4468, -0.0377, -0.0420,\n",
      "          0.2362,  0.1194]])\n"
     ]
    }
   ],
   "source": [
    "for images,labels in train_loader:\n",
    "    print(\"images.shape:\",images.shape)\n",
    "    outputs=model(images)\n",
    "    break\n",
    "    \n",
    "print(\"output.shape:\",outputs.shape)\n",
    "print(\"Sample outputs:\\n\",outputs[:2].data) #showing 2 outputs of first batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8182, 1.3819, 1.0023, 0.9914, 0.7001, 1.2070, 1.4549, 1.0539, 0.9679,\n",
       "        1.0839], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as the outputs are negative we need to use softmax to convaert them to probabilities\n",
    "exps=torch.exp(outputs[0]) \n",
    "exps\n",
    "\n",
    "\n",
    "#each output is replaced by e to the power output\n",
    "\n",
    "#now the numbers are not between 0 to 1 and also all probabilities should add up to one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0767, 0.1296, 0.0940, 0.0930, 0.0657, 0.1132, 0.1365, 0.0989, 0.0908,\n",
       "        0.1017], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs=exps/torch.sum(exps)   #then we calculate probabilities by dividing each outcome by the sum of all the outcomes \n",
    "probs\n",
    "\n",
    "#softmax formula\n",
    "#  e raise to outcome/ summation(e raise to outcomes)\n",
    "#this will generate the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities\n",
      " tensor([[0.0767, 0.1296, 0.0940, 0.0930, 0.0657, 0.1132, 0.1365, 0.0989, 0.0908,\n",
      "         0.1017],\n",
      "        [0.1169, 0.0955, 0.0933, 0.0742, 0.0667, 0.1472, 0.0907, 0.0903, 0.1192,\n",
      "         0.1061]])\n",
      "Sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "#now we use inbuilt function for softmax\n",
    "#apply softmax for each ouput row\n",
    "#we tell to apply softmax across  dimension one as the output shape is (128,10)\n",
    "#and the 0th dimension is the batch size so we  want to apply softmax across 1th dimention\n",
    "probs=F.softmax(outputs,dim=1)\n",
    "\n",
    "#look  at sample probabilities\n",
    "print(\"Sample probabilities\\n\",probs[:2].data)\n",
    "\n",
    "\n",
    "#Add up the probabilities of an output row\n",
    "print(\"Sum:\",torch.sum(probs[0]).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 5, 8, 3, 5, 8, 8, 8, 1, 2, 9, 0, 6, 8, 1, 8, 8, 5, 5, 2, 1, 8, 2, 5,\n",
      "        5, 5, 0, 5, 8, 7, 5, 5, 0, 8, 1, 1, 5, 8, 5, 8, 1, 8, 6, 8, 8, 0, 1, 6,\n",
      "        8, 5, 8, 9, 5, 8, 5, 1, 5, 6, 8, 8, 2, 1, 8, 2, 5, 8, 5, 6, 8, 5, 5, 5,\n",
      "        0, 5, 8, 8, 5, 8, 6, 6, 1, 2, 8, 3, 5, 5, 5, 8, 1, 5, 5, 8, 8, 8, 1, 8,\n",
      "        1, 5, 8, 8, 8, 8, 5, 2, 1, 1, 1, 1, 8, 6, 9, 2, 5, 8, 5, 0, 8, 1, 8, 8,\n",
      "        1, 5, 8, 8, 6, 1, 8, 8])\n",
      "tensor([0.1365, 0.1472, 0.1277, 0.1253, 0.1563, 0.1325, 0.1342, 0.1430, 0.1308,\n",
      "        0.1243, 0.1165, 0.1312, 0.1306, 0.1513, 0.1447, 0.1223, 0.1359, 0.1179,\n",
      "        0.1345, 0.1180, 0.1394, 0.1261, 0.1344, 0.1304, 0.1329, 0.1514, 0.1177,\n",
      "        0.1392, 0.1256, 0.1158, 0.1297, 0.1213, 0.1206, 0.1470, 0.1295, 0.1303,\n",
      "        0.1260, 0.1373, 0.1311, 0.1484, 0.1156, 0.1352, 0.1372, 0.1745, 0.1217,\n",
      "        0.1155, 0.1238, 0.1218, 0.1201, 0.1358, 0.1324, 0.1206, 0.1265, 0.1334,\n",
      "        0.1273, 0.1256, 0.1404, 0.1216, 0.1255, 0.1404, 0.1250, 0.1323, 0.1604,\n",
      "        0.1487, 0.1481, 0.1323, 0.1337, 0.1292, 0.1340, 0.1314, 0.1469, 0.1201,\n",
      "        0.1348, 0.1432, 0.1461, 0.1342, 0.1191, 0.1464, 0.1307, 0.1263, 0.1634,\n",
      "        0.1354, 0.1418, 0.1325, 0.1292, 0.1336, 0.1309, 0.1231, 0.1308, 0.1435,\n",
      "        0.1285, 0.1325, 0.1392, 0.1518, 0.1447, 0.1399, 0.1331, 0.1265, 0.1421,\n",
      "        0.1205, 0.1434, 0.1365, 0.1775, 0.1474, 0.1310, 0.1458, 0.1329, 0.1642,\n",
      "        0.1396, 0.1337, 0.1159, 0.1814, 0.1476, 0.1184, 0.1216, 0.1217, 0.1420,\n",
      "        0.1369, 0.1457, 0.1312, 0.1280, 0.1343, 0.1367, 0.1287, 0.1137, 0.1261,\n",
      "        0.1305, 0.1371], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs,preds=torch.max(probs,dim=1) \n",
    "#torch.max returns the largest element and the index of the largest element along a particular dimension\n",
    "#dim=1 as dimension 0 has batch size\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 1, 7, 2, 6, 4, 0, 8, 2, 2, 2, 7, 3, 7, 6, 9, 8, 9, 1, 2, 0, 4, 6, 2,\n",
       "        1, 6, 9, 2, 3, 9, 1, 1, 1, 3, 6, 3, 3, 8, 2, 0, 6, 9, 3, 5, 6, 7, 3, 0,\n",
       "        4, 6, 8, 4, 9, 5, 5, 1, 5, 6, 1, 5, 5, 0, 5, 9, 1, 8, 5, 6, 5, 9, 3, 7,\n",
       "        8, 6, 5, 5, 9, 0, 6, 3, 0, 7, 5, 4, 4, 5, 9, 7, 0, 1, 0, 9, 3, 7, 0, 5,\n",
       "        6, 2, 3, 4, 4, 8, 6, 0, 0, 5, 8, 0, 7, 8, 2, 0, 5, 9, 1, 1, 5, 6, 0, 9,\n",
       "        5, 4, 5, 4, 3, 5, 9, 7])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n",
    "#as we started with random  values of weights and biases prediction is very bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do element wise comparision between the prediction and labels\n",
    "#if element matches it stores a 1 else a zero and we sum this and divide by total number of elements in prediction\n",
    "\n",
    "def accuracy(outputs,labels):\n",
    "    _,preds=torch.max(outputs,dim=1)\n",
    "    return torch.tensor(torch.sum(preds==labels).item()/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1328)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the accuracy cannot be used as a loss function for optimizing our model using\n",
    "#gradient descent\n",
    "\n",
    "#1.its not a differentiable function torch.max and == are both non-continuous and non-differentiable\n",
    "#It doesn't take into account the actual probabilities predicted by the model so it can't\n",
    "#provide sufficient feedback for incremental improvements\n",
    "\n",
    "#like accuracy function cannot check that if probabilities of correct label increased but still the probability is\n",
    "#not the highest although the accuracy here as increased .These changes wont effect the accuracy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross Entropy\n",
    "\n",
    "#steps in cross entropy\n",
    "#for each output row pick the predicted probability for the correct label and ignore the rest\n",
    "\n",
    "#take the logarithm of the picked probability\n",
    "#if the probability is high  that is close to 1 then its logarithm is a very small negative value close to 0\n",
    "#and if the probability is low close to zero then the logarithm is a very large negative value\n",
    "#we multiply the result by -1 which results in a large positive value of the loss for poor predictions\n",
    "\n",
    "#finally take average of the cross entropy across all the ouput rows to get the overall loss for a batch\n",
    "\n",
    "#Mathematically\n",
    "\n",
    "#take ln of prediction \n",
    "#then create a matrix that has 1 at the highest probability index and 0 at the rest indexes\n",
    "#take the dot product of these 2 matrices\n",
    "#and then take the summation od all these cross entropy values for each row of output\n",
    "#and finally multiply with -1\n",
    "\n",
    "#this function is differentiable as we have created a vector which has all zeros and only 1 at the highest probability\n",
    "#and this allows us to multiply the two vectors\n",
    "\n",
    "#therefore cross entropy can be used as a loss funnction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3003, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Loss for current batch of data (cross_entropy automatically does the softmax function as well)\n",
    "loss=loss_fn(outputs,labels)\n",
    "print(loss)\n",
    "\n",
    "#the .item() method extracts the value as a python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):  #this class extends nn.Module\n",
    "    def __init__(self):       #the constructor\n",
    "        super().__init__()     #using super method to call constructor of nn.Module\n",
    "        self.linear=nn.Linear(input_size,num_classes)\n",
    "        #inside the constructor we instantiate the weights and biases\n",
    "        \n",
    "    def forward(self,xb):       #this method gets executed when we pass some input to the model\n",
    "        xb=xb.reshape(-1,784)\n",
    "        out=self.linear(xb)     #this invokes the above nn.Linear  \n",
    "        #this is similar to model(inputs) and it will return the output that is the prediction\n",
    "        return out\n",
    "    \n",
    "    def training_step(self,batch):\n",
    "        images,labels=batch\n",
    "        out=self(images)    #generate predictions        \n",
    "        #inside the model self is the model itself\n",
    "        #so we pass the images to the MnistModel\n",
    "        #and caluculate the loss\n",
    "        loss=F.cross_entropy(out,labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch):\n",
    "        images,labels=batch\n",
    "        out=self(images)               #generate predictions\n",
    "        loss=F.cross_entropy(out,labels)   #calculate loss\n",
    "        acc=accuracy(out,labels)             #calculate accuracy\n",
    "        #this function is defined earlier \n",
    "        return {\"val_loss\": loss,\"val_acc\": acc}\n",
    "    \n",
    "    def validation_epoch_end(self,outputs):\n",
    "        batch_losses=[x[\"val_loss\"] for x in outputs]\n",
    "        epoch_loss=torch.stack(batch_losses).mean()  #combine losses\n",
    "        #.stack() concates the tensor along a dimension \n",
    "        batch_accs=[x[\"val_acc\"] for x in outputs]\n",
    "        epoch_acc=torch.stack(batch_accs).mean()  #combine accuracies\n",
    "        \n",
    "        return {\"val_loss\": epoch_loss.item(),\"val_acc\": epoch_acc.item()}\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def epoch_end(self,epoch,result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f},val_acc: {:.4f}\".format(epoch,result[\"val_loss\"],result[\"val_acc\"]))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "model=MnistModel()\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,val_loader):\n",
    "    outputs=[model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3198418617248535, 'val_acc': 0.11609968543052673}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history=[]\n",
    "    optimizer=opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        #training phase\n",
    "        for batch in train_loader:\n",
    "            loss=model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()  #this performs the gradient desecent\n",
    "            optimizer.zero_grad()\n",
    "        #validation phase\n",
    "        result=evaluate(model,val_loader)\n",
    "        model.epoch_end(epoch,result)\n",
    "        history.append(result)\n",
    "    return history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3397204875946045, 'val_acc': 0.08306962251663208}"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.9700, val_acc: 0.5956\n",
      "Epoch [1], val_loss: 1.6956, val_acc: 0.7276\n",
      "Epoch [2], val_loss: 1.4900, val_acc: 0.7697\n",
      "Epoch [3], val_loss: 1.3346, val_acc: 0.7907\n",
      "Epoch [4], val_loss: 1.2151, val_acc: 0.8038\n"
     ]
    }
   ],
   "source": [
    "history1=fit(5,0.001,model,train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define test dataset\n",
    "test_dataset=MNIST(root='data/',train=False,transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28])\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img,label=test_dataset[0]\n",
    "plt.imshow(img[0],cmap='gray')\n",
    "print('Shape:',img.shape)\n",
    "print(\"Label:\",label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img,model):\n",
    "    xb=img.unsqueeze(0) #as aour model accepts batches unsqueeze adds an additional dimension for batch that is [1,1,28,28]\n",
    "    yb=model(xb)\n",
    "    _,preds=torch.max(yb,dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2 Predicted: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img,label=test_dataset[1839]\n",
    "plt.imshow(img[0],cmap=\"gray\")\n",
    "print(\"Label:\",label,\"Predicted:\",predict_image(img,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4 Predicted: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img,label=test_dataset[139]\n",
    "plt.imshow(img[0],cmap=\"gray\")\n",
    "print(\"Label:\",label,\"Predicted:\",predict_image(img,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 1.1929028034210205, 'val_acc': 0.80810546875}"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader=DataLoader(test_dataset,batch_size=256)\n",
    "result=evaluate(model,test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"mnist-logistic.pth\") #to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0254, -0.0036, -0.0161,  ..., -0.0288,  0.0328,  0.0219],\n",
       "                      [ 0.0073,  0.0261,  0.0116,  ...,  0.0006, -0.0071, -0.0029],\n",
       "                      [-0.0347, -0.0007, -0.0063,  ..., -0.0233,  0.0260, -0.0123],\n",
       "                      ...,\n",
       "                      [-0.0177,  0.0237,  0.0108,  ...,  0.0163,  0.0071,  0.0269],\n",
       "                      [-0.0310,  0.0175,  0.0283,  ...,  0.0174,  0.0078,  0.0331],\n",
       "                      [ 0.0126, -0.0225, -0.0345,  ...,  0.0056,  0.0282, -0.0055]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0012,  0.0236, -0.0163,  0.0196,  0.0263,  0.0423, -0.0276,  0.0434,\n",
       "                      -0.0119, -0.0238]))])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()  #contains the weights and biases to avoid training again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0254, -0.0036, -0.0161,  ..., -0.0288,  0.0328,  0.0219],\n",
       "                      [ 0.0073,  0.0261,  0.0116,  ...,  0.0006, -0.0071, -0.0029],\n",
       "                      [-0.0347, -0.0007, -0.0063,  ..., -0.0233,  0.0260, -0.0123],\n",
       "                      ...,\n",
       "                      [-0.0177,  0.0237,  0.0108,  ...,  0.0163,  0.0071,  0.0269],\n",
       "                      [-0.0310,  0.0175,  0.0283,  ...,  0.0174,  0.0078,  0.0331],\n",
       "                      [ 0.0126, -0.0225, -0.0345,  ...,  0.0056,  0.0282, -0.0055]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0012,  0.0236, -0.0163,  0.0196,  0.0263,  0.0423, -0.0276,  0.0434,\n",
       "                      -0.0119, -0.0238]))])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2=MnistModel()\n",
    "model2.load_state_dict(torch.load(\"mnist-logistic.pth\"))  #to load the stored weights and biases\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 1.1929028034210205, 'val_acc': 0.80810546875}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader=DataLoader(test_dataset,batch_size=256)\n",
    "result=evaluate(model2,test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
